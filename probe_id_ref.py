# -*- coding: utf-8 -*-
"""probe_ID_ref.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YDz2oxBenUCX9jg2HlpY6BbTwDFUBE3x
"""

import csv
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from google.colab import drive
drive.mount('/content/gdrive')

# TO CHECK IF THERE ARE HAVING ANY MISSING VALUES
def na(df):
  return print(np.where(np.asanyarray(pd.isna(df))))

# MERGING INPUT AND LABEL
def mergeIO(df):
  lf = pd.read_csv('/content/gdrive/My Drive/Masters/project/label.csv')
  label_map = {}
  for i in range(lf['ID_REF'].size):
    label_map[lf['ID_REF'][i]] = lf['class'][i]
  classes = []
  for i in range(df['ID_REF'].size):
    if df['ID_REF'][i] in label_map:
      classes.append(label_map[df['ID_REF'][i]])
    else:
      classes.append("")
  df['class'] = classes
  #na(df)
  df = df.drop(index=df[df['class'].isin(['POST', 'PRE'])].index).reset_index(drop=True)
  return df

# TO SAVE THE DATAFRAME INTO X AND Y .NPY FILES
def npy(df):
  X = df.drop(['ID_REF','class'] ,axis=1)
  y=df['class']
  X.to_numpy()
  y.to_numpy
  return X,y

"""***ORIGINAL DATASET***"""

df = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
d_df = df.T
#na(d_df)
d_df.to_csv('ID.csv', header=False)
df = pd.read_csv('ID.csv')
df=mergeIO(df)
df.to_csv('input_basic.csv', index=False)
X,y=npy(df)
np.save('X.npy',X)
np.save('y.npy',y)
df['class'].unique()

df['class'].unique()

label_counts = df['class'].value_counts()
label_counts

"""# ***COLUMN G***"""

# Finding the column G rows and make that into G_file
with open('/content/gdrive/My Drive/Masters/project/sen.csv', 'r') as input_file, open('G_file.csv', 'w', newline='') as output_file:
    reader = csv.reader(input_file)
    writer = csv.writer(output_file)
    for row in reader:
        if row[6]:
            writer.writerow(row)
def find_same_rows(data, column_index):
    same_rows = []
    current_value = None
    current_row_indices = []

    for i, row in enumerate(data):
        if current_value is None or row[column_index] != current_value:
            if current_row_indices and len(current_row_indices) > 1:
                same_rows.append(current_row_indices)
            current_value = row[column_index]
            current_row_indices = [i]
        else:
            current_row_indices.append(i)

    if current_row_indices and len(current_row_indices) > 1:
        same_rows.append(current_row_indices)
    return same_rows

def get_values_by_indices(data, indices, column_index):
    return [row[column_index] for i, row in enumerate(data) if i in indices]

with open("G_file.csv", "r") as file:
    reader = csv.reader(file)
    data = [row for row in reader]

same_rows = find_same_rows(data, 6)
same_values = [get_values_by_indices(data, indices, 13) for indices in same_rows]

# for values in same_values:
#     print(values)

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
id_map = {}
for id_list in same_values:
    new_id = id_list[0]
    for old_id in id_list:
        id_map[old_id] = new_id
#print(id_map)
df['ID_REF'] = df['ID_REF'].replace(id_map)
df[df.duplicated(['ID_REF'])]
grouped_df = df.groupby('ID_REF').mean()
grouped_df = grouped_df.reset_index()
# na(grouped_df)
transposed_df = grouped_df.T
# na(transposed_df)
transposed_df.to_csv('ID_transposed.csv', header=False)
df = pd.read_csv('ID_transposed.csv')
df=mergeIO(df)
df.to_csv('input.csv', index=False)
X,y=npy(df)
np.save('Xg.npy',X)
np.save('yg.npy',y)
df

"""# ***T-test***"""

import pandas as pd
from scipy.stats import ttest_ind
df = pd.read_csv('input_basic.csv')
h=df['ID_REF']
df=df.drop('ID_REF' ,axis=1)
# Split your data into two groups based on your output labels
group1 = df[df.iloc[:, -1] == 'NSCLC'].iloc[:, :-1]
group2 = df[df.iloc[:, -1] != 'NSCLC'].iloc[:, :-1]
# Perform the T-test for each feature
t_scores, p_values = ttest_ind(group1, group2)

# Create a DataFrame to store the results
results_df = pd.DataFrame({'Feature': df.columns[:-1], 'T-score': t_scores, 'P-value': p_values})

# Sort the results_df DataFrame in decreasing order based on the T-score column
sorted_results_df = results_df.sort_values(by='P-value', ascending=True)

# Print the sorted DataFrame
print(sorted_results_df)

# Get the top k probe_ids with highest P-values
k = 10
top_features = []
for i in range(k):
    max_index = sorted_results_df['P-value'].idxmax()
    top_feature = sorted_results_df.loc[max_index, 'Feature']
    top_features.append(top_feature)
    sorted_results_df = sorted_results_df.drop(max_index)

# Print the top k probe_ids with highest P-values
print("The top features are",top_features)

df_selected = df.drop(columns=[col for col in df.columns if col not in top_features])
df_selected.insert(0,"ID_REF",h)
df_selected.to_csv('inputp.csv', index=False)
df = pd.read_csv('inputp.csv')
df=mergeIO(df)
df.to_csv('input.csv', index=False)
X,y=npy(df)
np.save('Xp.npy',X)
np.save('yp.npy',y)
df

"""# ***K-Means***"""

df=pd.read_csv('input_basic.csv')
df = df.drop(['class'],axis=1)
df =df.set_index("ID_REF").T
k = 10

# perform K-means clustering on the data
kmeans = KMeans(n_clusters=k, random_state=0,n_init='auto')
kmeans.fit(df)

# predict the cluster assignments for each data point
cluster_assignments = kmeans.predict(df)

# transform the original data into a reduced set of features based on the centroids
X_reduced = kmeans.transform(df)

# create a dataframe with the reduced features and their values
reduced_feature_names = ['Cluster ' + str(i) for i in range(k)]
df_reduced = pd.DataFrame(X_reduced, columns=reduced_feature_names)

# add a column to the reduced dataframe for the cluster assignments
df_reduced['Cluster Assignment'] = cluster_assignments

# create a new dataframe with the reduced feature values and their respective feature names
df_reduced_named = pd.DataFrame(columns=['ID_REF'] + reduced_feature_names)
for i in range(df.shape[1]):
    feature_name = df.columns[i]
    feature_values = df.iloc[:,i]
    cluster_values = [feature_values[cluster_assignments == j].mean() for j in range(k)]
    row = [feature_name] + cluster_values
    df_reduced_named.loc[i] = row

# set the feature name column as the index of the dataframe
df_reduced_named.set_index('ID_REF', inplace=True)

# print the dataframe with the reduced features and their values
df_reduced_named.to_csv('Kreduced.csv', header=True)

df = pd.read_csv('Kreduced.csv')
df=mergeIO(df)
df.to_csv('kmeans.csv', index=False)
X,y=npy(df)
np.save('Xk.npy',X)
np.save('yk.npy',y)
df





df = pd.read_csv('kmeans.csv')
print(df)
X = df.drop(['ID_REF','class'] ,axis=1)
#X =X.dropna(axis=1,inplace=True)
y=df['class']
scaler = StandardScaler()
X_scale= scaler.fit_transform(X)
label_encoder = LabelEncoder()
label_encoder.fit(y)
y = label_encoder.transform(y)
np.where(np.asanyarray(np.isnan(X)))
#T-test
f_scores, p_values = f_classif(X, y)
top_features = []
k=1000
for i in range(k):
    top_features.append(X.columns[f_scores.argmax()])
    f_scores[f_scores.argmax()] = 0  # Set the maximum f-score to 0 to avoid selecting the same feature twice
print(top_features)
len(top_features)

"""# Connecting P and K"""

df = pd.read_csv('inputp.csv')
z=df['ID_REF']
print(df)
kdf=df.T
print(kdf)
kdf.to_csv('inputpk.csv', header=False)