# -*- coding: utf-8 -*-
"""GCN-normal approch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fZKjhoPBjB3Q_7yelWggUJe3kV-Qr3u
"""

!pip install torch_geometric

import pandas as pd
from google.colab import drive
drive.mount('/content/gdrive')
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
from torch_geometric.nn import SAGEConv
from torch_geometric.nn import GATConv
from torch_geometric.nn import GINConv

# TO CHECK IF THERE ARE HAVING ANY MISSING VALUES
def na(df):
  return print(np.where(np.asanyarray(pd.isna(df))))

# MERGING INPUT AND LABEL
def mergeIO(df):
  lf = pd.read_csv('/content/gdrive/My Drive/Masters/project/label.csv')
  label_map = {}
  for i in range(lf['ID_REF'].size):
    label_map[lf['ID_REF'][i]] = lf['class'][i]
  classes = []
  for i in range(df['ID_REF'].size):
    if df['ID_REF'][i] in label_map:
      classes.append(label_map[df['ID_REF'][i]])
    else:
      classes.append("")
  df['class'] = classes
  #na(df)
  df = df.drop(index=df[df['class'].isin(['POST', 'PRE'])].index).reset_index(drop=True)
  return df

# TO SAVE THE DATAFRAME INTO X AND Y .NPY FILES
def npy(df):
  X = df.drop(['ID_REF','class'] ,axis=1)
  y=df['class']
  X.to_numpy()
  y.to_numpy
  return X,y

def matrix_sparsity(matrix):
    total_elements = matrix.shape[0] * matrix.shape[1]
    non_zero_elements = (matrix != 0).sum()
    print('Number of non_zero elements',non_zero_elements)
    zero_elements = total_elements - non_zero_elements
    sparsity = (zero_elements / total_elements) * 100
    return sparsity

# Read in the data
sen = pd.read_csv('/content/gdrive/My Drive/Masters/project/sen.csv', low_memory=False)
df = pd.read_csv('/content/gdrive/My Drive/Masters/project/Book3.csv')
label= pd.read_csv('/content/gdrive/My Drive/Masters/project/label.csv')
l = sen[['Entrez_Gene_ID','Probe_Id']].dropna()

#Map entrez gene ID to Probe_ID
map_dict = l.set_index('Entrez_Gene_ID')['Probe_Id'].to_dict()
print('Number of probe_ID with entrez gene ID',len(map_dict))

# Filter the interactions to include only genes with Entrez IDs in sen.csv
entrez_ids = list(l['Entrez_Gene_ID'])
print(len(entrez_ids))
df = df[df['Entrez Gene Interactor A'].isin(entrez_ids) & df['Entrez Gene Interactor B'].isin(entrez_ids)]

# create empty graph
G = nx.Graph()

# add edges with weights
count=0
for i, row in df.iterrows():
    a = map_dict.get(row['Entrez Gene Interactor A'], None)
    b = map_dict.get(row['Entrez Gene Interactor B'], None)
    if a is not None and b is not None:
        G.add_edge(a, b, weight=1)
        #G.add_edge(b,a,weight=1)
        count+=1
print("Number of probe_ID pairs connected ",count)

# Convert the graph to undirected
G = G.to_undirected()

# Find the number of nodes
num_nodes = G.number_of_nodes()
print("Number of nodes in graph ", num_nodes) 

# get adjacency matrix
adj_matrix = nx.to_numpy_array(G)

# print adjacency matrix
print(adj_matrix)
print('Dimension of adjacency matrix is', adj_matrix.shape)

if np.array_equal(adj_matrix, adj_matrix.T):
    print("Matrix is symmetric, graph is undirected")

q=matrix_sparsity(adj_matrix)
print("The sparsity of adjacency matrix is:", q, "%")

# print nodes and edges
probe_ids=G.nodes()
print("Nodes: ", G.nodes())
print( "Number of nodes", len(G.nodes()))
print("Edges: ", G.edges())
print("Number of edges", len(G.edges()))

import matplotlib.pyplot as plt

# Draw the graph
pos = nx.spring_layout(G)
nx.draw(G, pos)

# Draw labels for nodes
labels = {n: n for n in G.nodes()}
nx.draw_networkx_labels(G, pos, labels, font_size=10)

# Show the plot
plt.show()

# Convert the graph to undirected
G_undirected = G.to_undirected()

# Get a list of connected components
components = list(nx.connected_components(G_undirected))

# Select a component with 20-30 nodes
selected_component = None
for component in components:
    if len(component) >= 20 and len(component) <= 30:
        selected_component = component
        break

# Create a subgraph with selected nodes
subgraph = G.subgraph(selected_component)

# Draw the subgraph
pos = nx.spring_layout(subgraph, k=0.5)
nx.draw(subgraph, pos, with_labels=True)
plt.show()

# import torch
# import torch_geometric.nn as nn
# import torch_geometric.utils as utils

# class GCN(torch.nn.Module):
#     def __init__(self, input_dim, hidden_dim, output_dim):
#         super(GCN, self).__init__()
#         self.conv1 = GCNConv(input_dim, hidden_dim)
#         self.conv2 = GCNConv(hidden_dim, output_dim)

#     def forward(self, x, edge_index):
#         x = self.conv1(x, edge_index)
#         x = F.relu(x)
#         x = self.conv2(x, edge_index)
#         return x


# # prepare the input data

# edge_index = utils.dense_to_sparse(torch.Tensor(adj_matrix))[0]

# x = torch.randn(1149, 1149) # random input features

# # create an instance of the GCN model and pass the input data to it
# input_dim = x.size(1)
# hidden_dim = 16
# output_dim = 1

# model = GCN(input_dim, hidden_dim, output_dim)
# output = model(x, edge_index)
# scores = output.detach().numpy()   # convert output to numpy array
# print(len(scores))
# sorted_indices = np.argsort(scores)[::-1]   # sort indices in descending order


# # compute PageRank scores for all nodes
page_ranks = nx.pagerank(G)

# get the indices of the top k nodes with highest PageRank scores
sorted_indices = np.argsort(list(page_ranks.values()))
node_names = {i: n for i, n in enumerate(map_dict.values())}
top_probe_ids = [node_names[i] for i in sorted_indices]
print(top_probe_ids)
print(len(top_probe_ids))

df = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
probe_ids = list(df["ID_REF"])
print(len(probe_ids))
probe_ids_adj=G.nodes()
print(len(probe_ids_adj))
present_probe_ids = []
for probe_id in probe_ids_adj:
    if probe_id in probe_ids:
        present_probe_ids.append(probe_id)
print(len(set(present_probe_ids)))
d_df = df.T
#na(d_df)
d_df.to_csv('ID.csv', header=False)
df = pd.read_csv('ID.csv')
df=mergeIO(df)
df.to_csv('input_basic.csv', index=False)
h=df['ID_REF']
df_selected = df.drop(columns=[col for col in df.columns if col not in present_probe_ids])
print(df_selected.shape)
df_selected.insert(0,"ID_REF",h)
df_selected.to_csv('inputp.csv', index=False)
df = pd.read_csv('inputp.csv')
df=mergeIO(df)
df.to_csv('df.csv', index=False)
X,y=npy(df)
np.save('Xgcn.npy',X)
np.save('ygcn.npy',y)
df

df = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
d_df = df.T
#na(d_df)
d_df.to_csv('ID.csv', header=False)
df = pd.read_csv('ID.csv')
df=mergeIO(df)
df.to_csv('input_basic.csv', index=False)
X,y=npy(df)
np.save('X.npy',X)
np.save('y.npy',y)
print(df['class'].value_counts())
df

import pandas as pd
import numpy as np

# Load the data into a pandas DataFrame, where each row represents a patient and each column represents a probe_id
data =  df

# Get the list of probe_ids that are present in the adjacency matrix
probe_ids = list(data.columns[1:-1])
print(len(probe_ids))
probe_ids_adj=G.nodes()
print(len(probe_ids_adj))
present_probe_ids = []
for probe_id in probe_ids_adj:
    if probe_id in probe_ids:
        present_probe_ids.append(probe_id)
print(len(set(present_probe_ids)))
# Create a new DataFrame with patient IDs as the index and present_probe_ids as the columns
data_with_patient_ids = pd.DataFrame(index=data["ID_REF"], columns=present_probe_ids)
for probe_id in present_probe_ids:
    data_with_patient_ids[probe_id] = data[data[probe_id]!=0][probe_id].values
#print(data_with_patient_ids)
import numpy as np

# Calculate mean and standard deviation of expression values for each patient
patient_mean = data.iloc[:,1:-1].mean(axis=1)
print(patient_mean)
patient_std = data.iloc[:,1:-1].std(axis=1)

# Create dictionary to store probe_id to patient_id mapping
probe_to_patient = {}

# Loop over each probe and check if expression values are within one standard deviation of mean for each patient
count = 0
for probe_id in present_probe_ids:
    probe_values = data[probe_id]
    for i, patient_id in enumerate(data.index):
        if np.abs(probe_values[i] - patient_mean[patient_id]) > patient_std[patient_id]:
            probe_to_patient[probe_id] = data.loc[patient_id, "ID_REF"]
            count += 1
            break
    # else:
    #     probe_to_patient[probe_id] = probe_id
print(count)
print(probe_to_patient)

keys = list(probe_to_patient.keys())

# create a mask of nodes that are in the set of probe_ids
mask = [probe_id in keys for probe_id in probe_ids_adj]

# use the mask to filter the adjacency matrix
adj_matrix_filtered = adj_matrix[mask][:, mask]
print(adj_matrix_filtered)
print(adj_matrix_filtered.shape)

matrix_sparsity(adj_matrix_filtered)

# get the node names of the filtered adjacency matrix
node_names = [probe_id for i, probe_id in enumerate(probe_ids_adj) if mask[i]]
# print the node names
print(node_names)

# get the patient IDs corresponding to each node in the adjacency matrix
patient_ids = [probe_to_patient[node] for node in node_names]
print(patient_ids)

# create a new adjacency matrix with patient IDs as node names
adj_matrix_patient = np.zeros((len(node_names), len(node_names)))
for i, node1 in enumerate(node_names):
    for j, node2 in enumerate(node_names):
        adj_matrix_patient[i, j] = adj_matrix_filtered[i, j]

# replace the node names with patient IDs
node_names_patient = patient_ids

# print the new adjacency matrix with patient IDs as node names
print(adj_matrix_patient)
print(adj_matrix_patient.shape)
print(node_names_patient)
#print(len(set(node_names_patient)))
counts={}
for val in node_names_patient:
  counts[val]=counts.get(val,0) +1
print(counts)

import pandas as pd
import numpy as np
import torch

# Read the input_basic.csv file
gene_data = pd.read_csv('input_basic.csv', index_col=0)

# Get the column headers of the gene_data dataframe
probe_ids_all = gene_data.drop('class', axis=1).columns.tolist()
print(len(probe_ids_all))

# Get the probe IDs present in the adjacency matrix
probe_ids_adj = list(G.nodes())
print(len(probe_ids_adj))

# Get the intersection between the two sets of probe IDs
probe_ids = list(set(probe_ids_all).intersection(set(probe_ids_adj)))
print(len(probe_ids))


# Filter the adjacency matrix based on the probe IDs present in the gene expression data
G_filtered = nx.Graph()
for u, v in G.edges():
    if u in probe_ids and v in probe_ids:
        G_filtered.add_edge(u, v)

# get adjacency matrix
adj_matrix = nx.to_numpy_array(G_filtered)
print(adj_matrix.shape)

# Get the probe IDs present in the adjacency matrix
probe_ids_adj = list(G_filtered.nodes())
print(len(probe_ids_adj))

# Get the intersection between the two sets of probe IDs
probe_ids = list(set(probe_ids_all).intersection(set(probe_ids_adj)))
print(len(probe_ids))

# Filter the gene_data dataframe based on the probe IDs present in the adjacency matrix
gene_data_filtered = gene_data.loc[:, probe_ids]

# Add the class column back to the filtered dataframe
gene_data_filtered = pd.concat([gene_data_filtered, gene_data.loc[:, "class"]], axis=1)

# extract the class labels
labels = gene_data_filtered.loc[:, 'class']

# Filter the gene_data dataframe based on the probe IDs present in the adjacency matrix
gene_data_filtered = gene_data.loc[:, probe_ids]

# convert the gene expression data to a numpy array
gene_data_filtered = gene_data_filtered.to_numpy()

# normalize the gene expression data
gene_data_filtered = (gene_data_filtered - gene_data_filtered.mean(axis=0)) / gene_data_filtered.std(axis=0)

# create a dictionary that maps probe IDs to indices
probe_id_to_index = {pid: i for i, pid in enumerate(gene_data_filtered[0])}

# create a list of feature vectors for each sample
features_list = []
for sample in gene_data_filtered[:, :]:
    feature_vec = [sample[probe_id_to_index[pid]] for pid in gene_data_filtered[0]]
    features_list.append(feature_vec)

# convert the feature vectors and labels to tensors
features = torch.tensor(features_list, dtype=torch.float32)
print(features)
print(features.shape)
label_to_number = {'NSCLC': 0, 'NHC': 1}
labels = labels.map(label_to_number)
labels = torch.tensor(labels.to_numpy(), dtype=torch.long)
labels = F.one_hot(labels, num_classes=2).to(torch.float32)
print(labels.shape)

import pandas as pd
import numpy as np

# Load the data from a CSV file
actual = df = pd.read_csv('input_basic.csv')

# Extract the gene expression values
gene_expr = actual.iloc[:, 1:-1].T

# Compute the correlation matrix
corr_matrix = gene_expr.corr(method="pearson") # or method="spearman"

# Apply a threshold to convert the correlation matrix to an adjacency matrix
threshold = 0.95
adj_matrix = np.where(abs(corr_matrix) > threshold, 1, 0)
print(adj_matrix)
print(matrix_sparsity(adj_matrix))
print(adj_matrix.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data
from torch.utils.data import Subset

class GCN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.sigmoid(x)

# Define the training function
def train(model, d, labels, optimizer, criterion, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        output = model(d.x, d.edge_index)
        patient_nodes = torch.tensor(patient_indices_train, dtype=torch.long)
        output = output[patient_nodes]
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

# Define the test function
def test(model, d, labels):
    model.eval()
    with torch.no_grad():
        output = model(d.x, d.edge_index)
        patient_nodes = torch.tensor(patient_indices_test, dtype=torch.long)
        output = output[patient_nodes]
        predicted_labels = (output > 0.5).float()
        accuracy = (predicted_labels == labels).float().mean()
    return accuracy

# Convert adj_matrix to a PyTorch tensor
adj_matrix = torch.tensor(adj_matrix,dtype=torch.float32).clone().detach().requires_grad_(True)
# Create a PyTorch geometric Data object
d = Data(x=features, edge_index=torch.nonzero(adj_matrix).t(), y=labels)
# d = Data(x=features.t(), edge_index=torch.transpose(torch.nonzero(adj_matrix).t(), 0, 1), y=labels)


# Split the data into training and testing sets
train_mask = torch.zeros(len(labels_t), dtype=torch.bool)
train_mask[:200] = True
test_mask = torch.zeros(len(labels_t), dtype=torch.bool)
test_mask[200:] = True

patient_indices_train=list(range(1,201))
patient_indices_test=list(range(201,256))

# Train the GCN model
model = GCN(num_features=1077, num_classes=2)
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.BCELoss()
train(model, d, labels, optimizer, criterion, num_epochs=100)

# Test the GCN model
accuracy = test(model, d, labels)
print("Test accuracy: {:.2f}%".format(accuracy * 100))

class GCN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.sigmoid(x)

# Define the training function
def train(model, data, labels, optimizer, criterion, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = criterion(output[data.train_mask], labels[data.train_mask])
        loss.backward()
        optimizer.step()

# Define the test function
def test(model, data, labels):
    model.eval()
    with torch.no_grad():
        output = model(data.x, data.edge_index)
        predicted_labels = (output > 0.5).float()
        accuracy = (predicted_labels[data.test_mask] == labels[data.test_mask]).float().mean()
    return accuracy

# Convert adj_matrix to a PyTorch tensor
adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)

# Create a PyTorch geometric Data object
data = Data(x=features, edge_index = torch.tensor(adj_matrix.nonzero().t(), dtype=torch.long), y=labels)
print(data)
print(adj_matrix.shape)
print(features.shape)
print(adj_matrix.shape[0] )
print(features.shape[1])
print(labels.shape)

# Split the data into training and test sets
train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)
train_mask[:200] = True
test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)
test_mask[200:] = True
data.train_mask = train_mask
data.test_mask = test_mask
print(data.train_mask.sum())
print(data.test_mask.sum())

# Train the GCN model
model = GCN(features.shape[1], num_classes=2)
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.BCELoss()
train(model, data, labels, optimizer, criterion, num_epochs=100)

# Test the GCN model
accuracy = test(model, data, labels)
print("Test accuracy: {:.2f}%".format(accuracy * 100))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data
from torch.utils.data import Subset

# Define the GraphSAGE model
class GraphSAGE(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(num_features, 16)
        self.conv2 = SAGEConv(16, num_classes)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Define the training function
def train(model, d, labels, optimizer, criterion, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        output = model(d.x, d.edge_index)
        loss = criterion(output[d.train_mask], labels[d.train_mask])
        loss.backward()
        optimizer.step()

# Define the test function
def test(model, d, labels):
    model.eval()
    with torch.no_grad():
        output = model(d.x, d.edge_index)
        predicted_labels = output.argmax(dim=1)
        accuracy = (predicted_labels[d.test_mask] == labels[d.test_mask]).float().mean()
    return accuracy

# Convert adj_matrix to a PyTorch tensor
adj_matrix = torch.tensor(adj_matrix,dtype=torch.float32).clone().detach().requires_grad_(True)
print(adj_matrix.shape)
print(features.shape)
print(adj_matrix.shape[0] )
print(features.shape[1])

print()
# Create a PyTorch geometric Data object
d = Data(x=features, edge_index=torch.transpose(torch.nonzero(adj_matrix), 0, 1), y=labels)
#print(d.edge_index)

# Split the data into training and test sets
train_mask = torch.zeros(d.num_nodes, dtype=torch.bool)
train_mask[[0, 2]] = True
test_mask = torch.zeros(d.num_nodes, dtype=torch.bool)
test_mask[[1, 3]] = True
d.train_mask = train_mask
d.test_mask = test_mask

# Train the GraphSAGE model
model = GraphSAGE(num_features=668, num_classes=2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()
train(model, d, labels, optimizer, criterion, num_epochs=100)

# Test the GraphSAGE model
accuracy = test(model, d, labels)
print("Test accuracy: {:.2f}%".format(accuracy * 100))