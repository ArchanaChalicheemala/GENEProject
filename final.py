# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fjOck5F73TrkJRh8loV6FvvFOVkKGGsQ
"""

!pip install torch_geometric
import pandas as pd
from google.colab import drive
drive.mount('/content/gdrive')
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
from torch_geometric.nn import SAGEConv
from torch_geometric.nn import GATConv
from torch_geometric.nn import GINConv

"""# ***DEFINED FUNCTIONS***"""

# TO CHECK IF THERE ARE HAVING ANY MISSING VALUES
def na(df):
  return print(np.where(np.asanyarray(pd.isna(df))))

# MERGING INPUT AND LABEL
def mergeIO(df):
  lf = pd.read_csv('/content/gdrive/My Drive/Masters/project/label.csv')
  label_map = {}
  for i in range(lf['ID_REF'].size):
    label_map[lf['ID_REF'][i]] = lf['class'][i]
  classes = []
  for i in range(df['ID_REF'].size):
    if df['ID_REF'][i] in label_map:
      classes.append(label_map[df['ID_REF'][i]])
    else:
      classes.append("")
  df['class'] = classes
  #na(df)
  df = df.drop(index=df[df['class'].isin(['POST', 'PRE'])].index).reset_index(drop=True)
  return df

# TO SAVE THE DATAFRAME INTO X AND Y .NPY FILES
def npy(df):
  X = df.drop(['ID_REF','class'] ,axis=1)
  y=df['class']
  X.to_numpy()
  y.to_numpy
  return X,y

def matrix_sparsity(matrix):
    total_elements = matrix.shape[0] * matrix.shape[1]
    non_zero_elements = (matrix != 0).sum()
    print('Number of non_zero elements',non_zero_elements)
    zero_elements = total_elements - non_zero_elements
    sparsity = (zero_elements / total_elements) * 100
    return sparsity

def admatrix(G):
  # get adjacency matrix
  adj_matrix = nx.to_numpy_array(G)
  np.fill_diagonal(adj_matrix, 1)
  # print adjacency matrix
  print(adj_matrix)
  print('Dimension of adjacency matrix is', adj_matrix.shape)
  return adj_matrix

"""# ***PROBE_IDS GRAPH FROM BIOGRID DATA***"""

# Read in the data
sen = pd.read_csv('/content/gdrive/My Drive/Masters/project/sen.csv', low_memory=False)
df = pd.read_csv('/content/gdrive/My Drive/Masters/project/Book3.csv')
label= pd.read_csv('/content/gdrive/My Drive/Masters/project/label.csv')
l = sen[['Entrez_Gene_ID','Probe_Id']].dropna()

#Map entrez gene ID to Probe_ID
map_dict = l.set_index('Entrez_Gene_ID')['Probe_Id'].to_dict()
print('Number of probe_ID with entrez gene ID',len(map_dict))

# Filter the interactions to include only genes with Entrez IDs in sen.csv
entrez_ids = list(l['Entrez_Gene_ID'])
print(len(entrez_ids))
df = df[df['Entrez Gene Interactor A'].isin(entrez_ids) & df['Entrez Gene Interactor B'].isin(entrez_ids)]

# create empty graph
G= nx.Graph()

# add edges with weights
count=0
for i, row in df.iterrows():
    a = map_dict.get(row['Entrez Gene Interactor A'], None)
    b = map_dict.get(row['Entrez Gene Interactor B'], None)
    if a is not None and b is not None:
        G.add_edge(a, b, weight=1)
        #G.add_edge(b,a,weight=1)
        count+=1
print("Number of probe_ID pairs connected ",count)

# Convert the graph to undirected
G = G.to_undirected()

# create an empty graph
G1 = nx.Graph()

# Get the list of probe_ids that are present in the adjacency matrix
data = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
probe_ids = list(data["ID_REF"])
print(len(probe_ids))
probe_ids_adj=G.nodes()
print(len(probe_ids_adj))
present_probe_ids = []
for probe_id in probe_ids_adj:
    if probe_id in probe_ids:
        present_probe_ids.append(probe_id)
print(len(set(present_probe_ids)))


# find the intersection of the two sets
common_probe_ids = list(set(probe_ids).intersection(set(probe_ids_adj)))

# add the common Probe IDs as nodes in G1
G1.add_nodes_from(common_probe_ids)

# add the edges from G that connect the common Probe IDs
for edge in G.edges():
    if edge[0] in common_probe_ids and edge[1] in common_probe_ids:
        G1.add_edge(edge[0], edge[1], weight=G.get_edge_data(edge[0], edge[1])['weight'])

# Find the number of nodes
num_nodes = G1.number_of_nodes()
print("Number of nodes in graph ", num_nodes) 

# get adjacency matrix
adj_matrix = nx.to_numpy_array(G1)
np.fill_diagonal(adj_matrix, 1)

# print adjacency matrix
print(adj_matrix)
print('Dimension of adjacency matrix is', adj_matrix.shape)

if np.array_equal(adj_matrix, adj_matrix.T):
    print("Matrix is symmetric, graph is undirected")

q=matrix_sparsity(adj_matrix)
print("The sparsity of adjacency matrix is:", q, "%")


# print nodes and edges
probe_ids=G1.nodes()
print("Nodes: ", G1.nodes())
print( "Number of nodes", len(G1.nodes()))
print("Edges: ", G1.edges())
print("Number of edges", len(G1.edges()))


#TO VIEW THE GRAPH
import matplotlib.pyplot as plt

# Draw the graph
pos = nx.spring_layout(G1)
nx.draw(G1, pos)

# Draw labels for nodes
labels = {n: n for n in G1.nodes()}
nx.draw_networkx_labels(G1, pos, labels, font_size=10)

# Show the plot
plt.show()


#GRAPH FOR LESS NUMBER OF NODES
# Convert the graph to undirected
G_undirected = G1.to_undirected()

# Get a list of connected components
components = list(nx.connected_components(G_undirected))

# Select a component with 20-30 nodes
selected_component = None
for component in components:
    if len(component) >= 20 and len(component) <= 30:
        selected_component = component
        break

# Create a subgraph with selected nodes
subgraph = G1.subgraph(selected_component)

# Draw the subgraph
pos = nx.spring_layout(subgraph, k=0.5)
nx.draw(subgraph, pos, with_labels=True)
plt.show()
matrix_sparsity(admatrix(G1))
adj_matrix=admatrix(G1)

df = pd.read_csv('/content/gdrive/My Drive/Masters/project/data.csv')
probe_ids = list(df["ID_REF"])
print(len(probe_ids))
probe_ids_adj=G.nodes()
print(len(probe_ids_adj))
present_probe_ids = []
for probe_id in probe_ids_adj:
    if probe_id in probe_ids:
        present_probe_ids.append(probe_id)
print(len(set(present_probe_ids)))
d_df = df.T
#na(d_df)
d_df.to_csv('ID.csv', header=False)
df = pd.read_csv('ID.csv')
df=mergeIO(df)
df.to_csv('input_basic.csv', index=False)
h=df['ID_REF']
df_selected = df.drop(columns=[col for col in df.columns if col not in present_probe_ids])
print(df_selected.shape)
df_selected.insert(0,"ID_REF",h)
df_selected.to_csv('inputp.csv', index=False)
df = pd.read_csv('inputp.csv')
df=mergeIO(df)
df.to_csv('df.csv', index=False)
X,y=npy(df)
np.save('Xgcn.npy',X)
np.save('ygcn.npy',y)

# Extract features (drop ID_REF and class columns)
feature_matrix = df.drop(columns=["ID_REF", "class"]).values
print()
x=feature_matrix[254, :] 
x = np.expand_dims(x, axis=1)
# Print shape of feature matrix
print("Shape of feature matrix:", x.shape)

#Labels
labels = df.loc[:, 'class']
label_to_number = {'NSCLC': 0, 'NHC': 1}
labels = labels.map(label_to_number)
labels = torch.tensor(labels.to_numpy(), dtype=torch.long)
#labels = labels.unsqueeze(1) 
# labels = F.one_hot(labels, num_classes=2).to(torch.float32)
print("Shape of label matrix:", labels.shape)

"""# ***GCN***"""

# Convert input matrices to PyTorch tensors
features = torch.tensor(feature_matrix, dtype=torch.float)
adj = torch.tensor(adj_matrix, dtype=torch.float)
labels = torch.tensor(labels, dtype=torch.float)
# Compute edge index from adjacency matrix
edge_index = torch.tensor(adj.nonzero(), dtype=torch.long).t().contiguous()
print(edge_index)

torch.manual_seed(10)
# Define the model architecture
class GCN(torch.nn.Module):
    def __init__(self, input_channels, hidden_channels, output_channels,MLP_dim,predict_input):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, output_channels)
        self.mlp1 = torch.nn.Linear(predict_input, MLP_dim)
        self.mlp2 = torch.nn.Linear(MLP_dim, int(MLP_dim/4))
        self.mlp3 = torch.nn.Linear(int(MLP_dim/4), 1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x= x.T
        x = self.mlp1(x)
        x = F.relu(x)
        x = self.mlp2(x)
        x = F.relu(x)
        x = self.mlp3(x)
        x = torch.sigmoid(x)
        return x

# Create PyTorch tensors
edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
x = torch.tensor(x, dtype=torch.float)
print(x.shape)
labels = torch.tensor(labels, dtype=torch.float)

# Create the model
model = GCN(input_channels=1, hidden_channels=1, output_channels=1,predict_input=822,MLP_dim=512)

# Define the loss and optimizer
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

output = model(x, edge_index)
print(output)
print(output.shape)

from sklearn.metrics import accuracy_score
# Extract features (drop ID_REF and class columns)
feature_matrix = df.drop(columns=["ID_REF", "class"]).values
print(feature_matrix.shape)
#Labels
labels = df.loc[:, 'class']
label_to_number = {'NSCLC': 0, 'NHC': 1}
labels = labels.map(label_to_number)
# labels = torch.tensor(labels.to_numpy(), dtype=torch.float)
labels=labels.to_numpy()
#labels = labels.unsqueeze(1) 
# labels = F.one_hot(labels, num_classes=2).to(torch.float32)
print("Shape of label matrix:", labels.shape)



accuracy=[]
for i in range(255):
  x=feature_matrix[i, :] 
  x = np.expand_dims(x, axis=1)
  # Print shape of feature matrix
  # print("Shape of feature matrix:", x.shape)
  # Create PyTorch tensors
  edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
  x = torch.tensor(x, dtype=torch.float)
  # print("Shape of x matrix:",x.shape)
  y = torch.tensor(labels[i].reshape(1, 1), dtype=torch.float)
  # print("Shape of labels matrix:",y.shape)
  
  # Create the model
  model = GCN(input_channels=1, hidden_channels=1, output_channels=1,predict_input=822,MLP_dim=512)

  # Define the loss and optimizer
  criterion = torch.nn.BCEWithLogitsLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
  # Train the model
  model.train()
  for epoch in range(10):
      optimizer.zero_grad()
      output = model(x, edge_index)
      loss = criterion(output, y)
      loss.backward()
      optimizer.step()

  # Test the model
  model.eval()
  with torch.no_grad():
      output = model(x, edge_index)
      predicted_labels = torch.sigmoid(output).round()
      acc = accuracy_score(predicted_labels, y)
      accuracy.append(acc)
print(accuracy)
print('Accuracy:', sum(accuracy)/255)

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42,shuffle=True)

# Iterate through training set
accuracy = []
for i in range(X_test.shape[0]):
  x = X_train[i, :]
  x = np.expand_dims(x, axis=1)

  edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
  x = torch.tensor(x, dtype=torch.float)
  y = torch.tensor(y_train[i].reshape(1, 1), dtype=torch.float)

  # Create the model and define the loss and optimizer
  model = GCN(input_channels=1, hidden_channels=1, output_channels=1, predict_input=822, MLP_dim=512)
  criterion = torch.nn.BCEWithLogitsLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

  # Train the model
  model.train()
  for epoch in range(100):
      optimizer.zero_grad()
      output = model(x, edge_index)
      loss = criterion(output, y)
      loss.backward()
      optimizer.step()

  # Test the model
  model.eval()
  with torch.no_grad():
      x_test = np.expand_dims(X_test[i, :], axis=1)
      x_test = torch.tensor(x_test, dtype=torch.float)
      y_test_pred = model(x_test, edge_index)
      y_test_pred = torch.sigmoid(y_test_pred).int()
      acc = accuracy_score(y_test_pred, y_test[i].reshape(1, 1))
      accuracy.append(acc)
print(accuracy)
print('Accuracy:', sum(accuracy)*100/X_test.shape[0])

from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

# Define the number of splits for k-fold cross-validation
k = 10

# Create the k-fold cross-validator
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Initialize a list to store the accuracy of each fold
accuracy_list = []

# Loop over each fold
for fold, (train_idx, test_idx) in enumerate(kf.split(feature_matrix, labels)):
  
  # Get the training and testing sets for this fold
  X_train, X_test = feature_matrix[train_idx], feature_matrix[test_idx]
  y_train, y_test = labels[train_idx], labels[test_idx]
  
  # Iterate through training set
  accuracy = []
  for i in range(X_test.shape[0]):
    x = X_train[i, :]
    x = np.expand_dims(x, axis=1)

    edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
    x = torch.tensor(x, dtype=torch.float)
    y = torch.tensor(y_train[i].reshape(1, 1), dtype=torch.float)

    # Create the model and define the loss and optimizer
    model = GCN(input_channels=1, hidden_channels=1, output_channels=1, predict_input=822, MLP_dim=512)
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # Train the model
    model.train()
    for epoch in range(100):
        optimizer.zero_grad()
        output = model(x, edge_index)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

    # Test the model
    model.eval()
    with torch.no_grad():
        x_test = np.expand_dims(X_test[i, :], axis=1)
        x_test = torch.tensor(x_test, dtype=torch.float)
        y_test_pred = model(x_test, edge_index)
        y_test_pred = torch.sigmoid(y_test_pred).int()
        acc = accuracy_score(y_test_pred, y_test[i].reshape(1, 1))
        accuracy.append(acc)
  
  # Compute the average accuracy of this fold and add it to the list of accuracies
  fold_accuracy = sum(accuracy) / len(accuracy)
  accuracy_list.append(fold_accuracy)

# Compute the mean and standard deviation of the accuracies across all folds
mean_accuracy = np.mean(accuracy_list)
std_accuracy = np.std(accuracy_list)

print(f"Mean Accuracy: {mean_accuracy:.2%} Â± {std_accuracy:.2%}")

# Train the model
model.train()
for epoch in range(10):
    optimizer.zero_grad()
    output = model(x, edge_index)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()

# Test the model
model.eval()
with torch.no_grad():
    output = model(x, edge_index)
    predicted_labels = torch.sigmoid(output).round()
    accuracy = torch.sum(predicted_labels == labels) / labels.shape[0]
    print('Accuracy:', accuracy.item())

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

# Define the model architecture
class GCN(torch.nn.Module):
    def __init__(self, input_channels, hidden_channels, output_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, output_channels)
        self.mlp1 = torch.nn.Linear(output_channels, output_channels)
        self.mlp2 = torch.nn.Linear(output_channels, 255)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.mlp1(x)
        x = F.relu(x)
        x = self.mlp2(x)
        x = F.relu(x)
        x = torch.sum(x, dim=0)
        return x

# Create PyTorch tensors
edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
x = torch.tensor(feature_matrix.T, dtype=torch.float)
labels = torch.tensor(labels.squeeze(-1), dtype=torch.float)

# Create the model
model = GCN(input_channels=255, hidden_channels=64, output_channels=32)

# Define the loss and optimizer
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Train the model
model.train()
for epoch in range(10):
    optimizer.zero_grad()
    output = model(x, edge_index)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()

# Test the model
model.eval()
with torch.no_grad():
    output = model(x, edge_index)
    predicted_labels = torch.sigmoid(output).round()
    accuracy = torch.sum(predicted_labels == labels) / labels.shape[0]
    print('Accuracy:', accuracy.item())

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

# Define the model architecture
class GCN(torch.nn.Module):
    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_channels, hidden_channels)
        self.layers = torch.nn.ModuleList()
        for i in range(num_layers-1):
            self.layers.append(GCNConv(hidden_channels, hidden_channels))
        self.conv2 = GCNConv(hidden_channels, output_channels)
        self.mlp1 = torch.nn.Linear(output_channels, output_channels)
        self.mlp2 = torch.nn.Linear(output_channels, 255)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        for layer in self.layers:
            x = layer(x, edge_index)
            x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.mlp1(x)
        x = F.relu(x)
        x = self.mlp2(x)
        x = F.relu(x)
        x = torch.sum(x, dim=0)
        return x

# Create PyTorch tensors
edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)
x = torch.tensor(feature_matrix.T, dtype=torch.float)
labels = torch.tensor(labels.squeeze(-1), dtype=torch.float)

# Define the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=123)

for hidden_layer in [8, 16, 64, 128, 256]:
    for num_layer in [1, 2, 3]:
        print(f"Training GCN with {num_layer} layers and {hidden_layer} hidden units")
        total_acc = 0
        for i, (train_index, test_index) in enumerate(kf.split(labels)):
            # Create train and test masks
            train_mask = torch.zeros(len(labels), dtype=torch.bool)
            train_mask[train_index] = True
            test_mask = torch.zeros(len(labels), dtype=torch.bool)
            test_mask[test_index] = True

            # Create the model
            model = GCN(input_channels=255, hidden_channels=hidden_layer, output_channels=32, num_layers=num_layer)

            # Define the loss and optimizer
            criterion = torch.nn.BCEWithLogitsLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)

            # Train the model
            model.train()
            for epoch in range(100):
                optimizer.zero_grad()
                output = model(x, edge_index)
                loss = criterion(output[train_mask], labels[train_mask])
                loss.backward()
                optimizer.step()

            # Test the model
            model.eval()
            with torch.no_grad():
                output = model(x, edge_index)
                predicted_labels = (output[test_mask]> 0.5).int()
                accuracy = accuracy_score(predicted_labels,labels[test_mask])
                total_acc += accuracy.item()
        
        avg_acc = total_acc / 10
        print(f"Average Accuracy: {avg_acc*100:.4f}")