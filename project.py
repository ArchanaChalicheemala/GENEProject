# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTr5m5wX9AD0EqIW0g1Vyhg1I6vle_BQ
"""

import csv
import pandas as pd
import numpy as np
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/gdrive')

# Finding the column G rows and make thhat into G_file
with open('/content/gdrive/My Drive/project/sen.csv', 'r') as input_file, open('G_file.csv', 'w', newline='') as output_file:
    reader = csv.reader(input_file)
    writer = csv.writer(output_file)
    for row in reader:
        if row[6]:
            writer.writerow(row)
def find_same_rows(data, column_index):
    same_rows = []
    current_value = None
    current_row_indices = []

    for i, row in enumerate(data):
        if current_value is None or row[column_index] != current_value:
            if current_row_indices and len(current_row_indices) > 1:
                same_rows.append(current_row_indices)
            current_value = row[column_index]
            current_row_indices = [i]
        else:
            current_row_indices.append(i)

    if current_row_indices and len(current_row_indices) > 1:
        same_rows.append(current_row_indices)
    return same_rows

def get_values_by_indices(data, indices, column_index):
    return [row[column_index] for i, row in enumerate(data) if i in indices]

with open("G_file.csv", "r") as file:
    reader = csv.reader(file)
    data = [row for row in reader]

same_rows = find_same_rows(data, 6)
same_values = [get_values_by_indices(data, indices, 13) for indices in same_rows]

#for values in same_values:
    #print(values)

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('/content/gdrive/My Drive/project/data.csv')
id_map = {}
for id_list in same_values:
    new_id = id_list[0]
    for old_id in id_list:
        id_map[old_id] = new_id
#print(id_map)
df['ID_REF'] = df['ID_REF'].replace(id_map)
df[df.duplicated(['ID_REF'])]
grouped_df = df.groupby('ID_REF').mean()
grouped_df = grouped_df.reset_index()
print(np.where(np.asanyarray(pd.isna(grouped_df))))
grouped_df.to_csv('ID_grouped.csv', index=False)
transposed_df = grouped_df.T
print(np.where(np.asanyarray(pd.isna(transposed_df))))
transposed_df.to_csv('ID_transposed.csv', header=False)

##Merging imput and label
lf = pd.read_csv('/content/gdrive/My Drive/project/label.csv')
label_map = {}
for i in range(lf['ID_REF'].size):
  label_map[lf['ID_REF'][i]] = lf['class'][i]
df = pd.read_csv('ID_transposed.csv')
#df = transposed_df
classes = []
for i in range(df['ID_REF'].size):
  if df['ID_REF'][i] in label_map:
    classes.append(label_map[df['ID_REF'][i]])
  else:
    classes.append("")
df['class'] = classes
print(np.where(np.asanyarray(pd.isna(df))))
df.to_csv('input.csv', index=False)

df = pd.read_csv('/content/gdrive/My Drive/project/data.csv')
transposed_df = df.T
print(np.where(np.asanyarray(pd.isna(transposed_df))))
transposed_df.to_csv('ID_transposed.csv', header=False)

##Merging imput and label
lf = pd.read_csv('/content/gdrive/My Drive/project/label.csv')
label_map = {}
for i in range(lf['ID_REF'].size):
  label_map[lf['ID_REF'][i]] = lf['class'][i]
df = pd.read_csv('ID_transposed.csv')
#df = transposed_df
classes = []
for i in range(df['ID_REF'].size):
  if df['ID_REF'][i] in label_map:
    classes.append(label_map[df['ID_REF'][i]])
  else:
    classes.append("")
df['class'] = classes
print(np.where(np.asanyarray(pd.isna(df))))
df.to_csv('input_basic.csv', index=False)

"""T-test"""

df = pd.read_csv('input_basic.csv')
X = df.drop(['ID_REF','class'] ,axis=1)
#X =X.dropna(axis=1,inplace=True)
y=df['class']
scaler = StandardScaler()
X_scale= scaler.fit_transform(X)
label_encoder = LabelEncoder()
label_encoder.fit(y)
y = label_encoder.transform(y)
np.where(np.asanyarray(np.isnan(X)))
#T-test
f_scores, p_values = f_classif(X, y)
top_features = []
k=10000
for i in range(k):
    top_features.append(X.columns[f_scores.argmax()])
    f_scores[f_scores.argmax()] = 0  # Set the maximum f-score to 0 to avoid selecting the same feature twice
print(top_features)
len(top_features)

df_selected = df.drop(columns=[col for col in df.columns if col not in top_features])
X_selected = df.drop(['ID_REF','class'] ,axis=1)
y_selected=df['class']
# label_encoder = LabelEncoder()
# label_encoder.fit(y_selected)
# y_selected = label_encoder.transform(y_selected)
X.to_numpy()
#y.to_numpy()
np.save('Xp10000.npy',X)
np.save('yp10000.npy',y)

"""K-Means"""

df = pd.read_csv('ID_grouped.csv')
df
df = df.drop('ID_REF',axis=1)
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.20,shuffle=True,random_state=10)
# from sklearn.cluster import KMeans
# kmeans = KMeans(n_clusters=100, random_state=0)
# kmeans.fit(df)

import pandas as pd
from sklearn.cluster import KMeans

df = pd.read_csv('datakmeans.csv')
df = df.drop('ID_REF',axis=1)
k = 10000

# perform K-means clustering on the data
kmeans = KMeans(n_clusters=k, random_state=0)
kmeans.fit(df)

# predict the cluster assignments for each data point
cluster_assignments = kmeans.predict(df)

# transform the original data into a reduced set of features based on the centroids
X_reduced = kmeans.transform(df)

# create a dataframe with the reduced features and their values
reduced_feature_names = ['Cluster ' + str(i) for i in range(k)]
df_reduced = pd.DataFrame(X_reduced, columns=reduced_feature_names)

# add a column to the reduced dataframe for the cluster assignments
df_reduced['Cluster Assignment'] = cluster_assignments

# create a new dataframe with the reduced feature values and their respective feature names
df_reduced_named = pd.DataFrame(columns=['Feature Name'] + reduced_feature_names)
for i in range(df.shape[1]):
    feature_name = df.columns[i]
    feature_values = df.iloc[:,i]
    cluster_values = [feature_values[cluster_assignments == j].mean() for j in range(k)]
    row = [feature_name] + cluster_values
    df_reduced_named.loc[i] = row

# set the feature name column as the index of the dataframe
df_reduced_named.set_index('Feature Name', inplace=True)

# print the dataframe with the reduced features and their values
df_reduced_named.to_csv('Kreduced.csv', header=True)

##Merging imput and label
lf = pd.read_csv('/content/gdrive/My Drive/project/label.csv')
label_map = {}
for i in range(lf['ID_REF'].size):
  label_map[lf['ID_REF'][i]] = lf['class'][i]
df = pd.read_csv('Kreduced.csv')
#df = transposed_df
print(df)
classes = []
for i in range(df['Feature Name'].size):
  if df['Feature Name'][i] in label_map:
    classes.append(label_map[df['Feature Name'][i]])
  else:
    classes.append("")
df['class'] = classes
print(np.where(np.asanyarray(pd.isna(df))))
df.to_csv('kmeans.csv', index=False)

df

X = df.drop(['Feature Name','class'] ,axis=1)
#X =X.dropna(axis=1,inplace=True)
y=df['class']
X.to_numpy()
y.to_numpy
np.save('X_k_10000.npy',X)
np.save('y_k_10000.npy',y)